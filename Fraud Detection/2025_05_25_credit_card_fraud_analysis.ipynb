{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T20:00:39.863478Z",
          "iopub.status.busy": "2025-05-25T20:00:39.863042Z",
          "iopub.status.idle": "2025-05-25T20:00:39.871038Z",
          "shell.execute_reply": "2025-05-25T20:00:39.870034Z",
          "shell.execute_reply.started": "2025-05-25T20:00:39.863449Z"
        },
        "id": "JyCHReiYyWvX",
        "outputId": "9199b621-2808-44e2-991f-2d8b8990123b",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.12.7' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import ttest_ind\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import os\n",
        "\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM7JG6lryWvX"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:05.928348Z",
          "iopub.status.busy": "2025-05-25T19:31:05.928031Z",
          "iopub.status.idle": "2025-05-25T19:31:08.469216Z",
          "shell.execute_reply": "2025-05-25T19:31:08.468437Z",
          "shell.execute_reply.started": "2025-05-25T19:31:05.928317Z"
        },
        "id": "HTNPEKwdyWvY",
        "outputId": "e56cd266-a2bb-4bbe-f467-d5d2fdaef858",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('/Users/victoriayuzova/Data-Science-Projects/Fraud Detection/creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLH3udZRyWvY"
      },
      "source": [
        "# Explore dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcF2ZZcGCfvL"
      },
      "source": [
        "Checking some basic things:\n",
        "- if there are nulls,\n",
        "- if there any interesting correlations between variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:12.215837Z",
          "iopub.status.busy": "2025-05-25T19:31:12.215448Z",
          "iopub.status.idle": "2025-05-25T19:31:12.257756Z",
          "shell.execute_reply": "2025-05-25T19:31:12.256899Z",
          "shell.execute_reply.started": "2025-05-25T19:31:12.215812Z"
        },
        "id": "fPywk3SwyWvY",
        "outputId": "232e9332-9bef-47d8-d987-fdd15114c0c9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# all the variables are numeric, no need to worry about preprocessing categorical vars\n",
        "# no need to worry about nulls (some algoritms dont tolerate them)\n",
        "# good number of records\n",
        "# time and amount variables are the only variables not encrypted\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:15.532129Z",
          "iopub.status.busy": "2025-05-25T19:31:15.531845Z",
          "iopub.status.idle": "2025-05-25T19:31:15.979111Z",
          "shell.execute_reply": "2025-05-25T19:31:15.978240Z",
          "shell.execute_reply.started": "2025-05-25T19:31:15.532110Z"
        },
        "id": "QsK5NL4yyWvZ",
        "outputId": "18ea36ec-fcea-4292-c02c-994c96454f87",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# dont see much here, but since these are numeric, let´s plot some correlations and cehck distributions of variables\n",
        "\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oWofprZgEfCF",
        "outputId": "5d678695-2d79-4f10-bb7d-c4d38f1e5634"
      },
      "outputs": [],
      "source": [
        "df.hist(figsize=(12, 10))\n",
        "\n",
        "# time is bimodal\n",
        "# here I see that some variables are skewed or have outliers; anomalies are likely signals of fraud\n",
        "\n",
        "# not sure though what I can do about any of those distributions; further I will be tranforming them with StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:18.433784Z",
          "iopub.status.busy": "2025-05-25T19:31:18.433501Z",
          "iopub.status.idle": "2025-05-25T19:31:18.442713Z",
          "shell.execute_reply": "2025-05-25T19:31:18.441866Z",
          "shell.execute_reply.started": "2025-05-25T19:31:18.433765Z"
        },
        "id": "NYlSaMZxyWvZ",
        "outputId": "e58f0fcf-fd5b-4dd5-a5f7-73154ffbb631",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# the dataset is imbalanced\n",
        "\n",
        "df['Class'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['time_bin'] = pd.cut(df['Time'], bins=50)\n",
        "\n",
        "\n",
        "df.groupby('time_bin')['Class'].mean().reset_index().plot(\n",
        "    x='time_bin', y='Class', kind='line', figsize=(14, 4), alpha=0.5, title='Fraud Rate (%) Over Time'\n",
        ")\n",
        "\n",
        "df[df['Class'] == 1].groupby('time_bin')['Class'].count().reset_index().plot(\n",
        "    x='time_bin', y='Class', kind='line', figsize=(14, 4), alpha=0.5, title='Fraud Rate (count) Over Time'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:20.341250Z",
          "iopub.status.busy": "2025-05-25T19:31:20.340923Z",
          "iopub.status.idle": "2025-05-25T19:31:23.131487Z",
          "shell.execute_reply": "2025-05-25T19:31:23.130271Z",
          "shell.execute_reply.started": "2025-05-25T19:31:20.341226Z"
        },
        "id": "DN02OiHyyWvZ",
        "outputId": "c05b66d5-42dd-4e44-c674-42fe96139f05",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# let´s check if there are any features that are highly correlated\n",
        "# looks like there are no particularly strong correlations between variables\n",
        "df.drop('time_bin', axis=1, inplace=True)\n",
        "\n",
        "corr_matrix = df.corr().round(2)\n",
        "\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_downasampled = pd.concat([df[df['Class'] == 0].sample(492,random_state=42), df[df['Class'] == 1]])\n",
        "df_downasampled['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# having downsampled the dataset, we can better see the correlation between features because fraud cases are more visible\n",
        "# we will only use downsampling for analysis, not for modeling\n",
        "\n",
        "corr_matrix = df_downasampled.corr().round(2)\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.show()\n",
        "\n",
        "# Conclusions:\n",
        "## Features with highest positive corr to fraud are 2, 4, 11, 19\n",
        "## Features with highest negative corr to fraud are 3, 9, 10, 12, 14, 17, 18, 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdMCpEmxyWva"
      },
      "source": [
        "Correlation might not be the best when we have a binary variable and continuous feature. Let´s try doing t-test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:29.200427Z",
          "iopub.status.busy": "2025-05-25T19:31:29.200112Z",
          "iopub.status.idle": "2025-05-25T19:31:29.345625Z",
          "shell.execute_reply": "2025-05-25T19:31:29.344787Z",
          "shell.execute_reply.started": "2025-05-25T19:31:29.200383Z"
        },
        "id": "HPaNcvccyWva",
        "outputId": "61dfeb8c-101b-4d27-d52e-fa769d1ea8c0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "not_fraud = df_downasampled[df_downasampled['Class'] == 0]\n",
        "fraud = df_downasampled[df_downasampled['Class'] == 1]\n",
        "\n",
        "train_cols = [col for col in df_downasampled.columns if col != 'Class'] # choosing all continuous columns except target\n",
        "\n",
        "# Run t-test for each feature\n",
        "\n",
        "p_values = []\n",
        "\n",
        "for c in train_cols:\n",
        "    p = ttest_ind(not_fraud[c], fraud[c]).pvalue\n",
        "    p_values.append({'variable': c, 'p_value': p})\n",
        "\n",
        "p_values_df = pd.DataFrame(p_values)\n",
        "p_values_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:33.457327Z",
          "iopub.status.busy": "2025-05-25T19:31:33.456552Z",
          "iopub.status.idle": "2025-05-25T19:31:33.467766Z",
          "shell.execute_reply": "2025-05-25T19:31:33.466881Z",
          "shell.execute_reply.started": "2025-05-25T19:31:33.457301Z"
        },
        "id": "yqRqDQ_yyWva",
        "outputId": "3f9f4ca3-ae26-4864-9a56-fc7926b8964b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ok, here I treid to exclude the variables that we not relevant but seems like every variable is valuable here\n",
        "\n",
        "relevant_p_values = p_values_df[p_values_df['p_value']< 0.05]\n",
        "\n",
        "relevant_p_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NoSCdbByWva"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ktTaeAyWva"
      },
      "source": [
        "First lets try a simple model (Logistic Regression) and let it handle the imbalanced class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-25T19:31:36.958326Z",
          "iopub.status.busy": "2025-05-25T19:31:36.958020Z",
          "iopub.status.idle": "2025-05-25T19:31:36.986105Z",
          "shell.execute_reply": "2025-05-25T19:31:36.985198Z",
          "shell.execute_reply.started": "2025-05-25T19:31:36.958305Z"
        },
        "id": "Dj8hAPfYyWvb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# split target and features\n",
        "\n",
        "# sorting df by time to later apply TimeSeries CV\n",
        "df = df.sort_values(by='Time')\n",
        "\n",
        "\n",
        "X = df.drop(['Time','Class'], axis=1)  # features\n",
        "y = df[\"Class\"]               # target\n",
        "\n",
        "# split into train and test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.25, random_state=42)\n",
        "\n",
        "#check sample size\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-25T19:32:27.283516Z",
          "iopub.status.busy": "2025-05-25T19:32:27.283145Z",
          "iopub.status.idle": "2025-05-25T19:32:27.498647Z",
          "shell.execute_reply": "2025-05-25T19:32:27.497731Z",
          "shell.execute_reply.started": "2025-05-25T19:32:27.283491Z"
        },
        "id": "FDSq0FIFyWvb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# With StandardScaler we need to bring different numerical variables to comparable scales before ingesting it into the model\n",
        "\n",
        "std = StandardScaler()\n",
        "std = std.fit(X_train)\n",
        "\n",
        "X_train = std.transform(X_train)\n",
        "X_test = std.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkJwUerdyWvb"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:32:44.132584Z",
          "iopub.status.busy": "2025-05-25T19:32:44.132218Z",
          "iopub.status.idle": "2025-05-25T19:32:48.596207Z",
          "shell.execute_reply": "2025-05-25T19:32:48.595346Z",
          "shell.execute_reply.started": "2025-05-25T19:32:44.132561Z"
        },
        "id": "4M4kn7u2yWvb",
        "outputId": "8290795a-3fe0-4e35-aa7b-0315b6bac4ae",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# here we´ll initialize the Time Serires Cross Validation\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=10)\n",
        "filtered_splits = []\n",
        "\n",
        "for i, (train_idx, val_idx) in enumerate(tscv.split(X_train), start=1):\n",
        "    y_val = y_train.iloc[val_idx]\n",
        "    positives = (y_val == 1).sum()\n",
        "    \n",
        "    if positives > 0:\n",
        "        filtered_splits.append((train_idx, val_idx))\n",
        "        print(f\"✅ Fold {i} kept: Positives = {positives}\")\n",
        "    else:\n",
        "        print(f\"❌ Fold {i} skipped: Positives = {positives}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFRhIWfHyWvb"
      },
      "source": [
        "Logistic Regression with Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "execution": {
          "iopub.execute_input": "2025-05-25T19:35:51.543153Z",
          "iopub.status.busy": "2025-05-25T19:35:51.542828Z",
          "iopub.status.idle": "2025-05-25T19:36:29.034851Z",
          "shell.execute_reply": "2025-05-25T19:36:29.031550Z",
          "shell.execute_reply.started": "2025-05-25T19:35:51.543128Z"
        },
        "id": "SLad6oB3yWvb",
        "outputId": "848010c0-dd39-4b22-ce80-183b3105cadd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "pipe = Pipeline([\n",
        "    ('ros', RandomOverSampler(random_state=42)),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid = {'clf__C':[0.001,0.01,0.1],'clf__class_weight':[None,'balanced']}\n",
        "search_lr = GridSearchCV(pipe, param_grid, cv=filtered_splits, scoring='average_precision', n_jobs=-1)\n",
        "search_lr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best PR AUC (mean CV):\", search_lr.best_score_)\n",
        "print(\"Best parameters:\", search_lr.best_params_)\n",
        "\n",
        "# Average precision on the full training set with the best model\n",
        "y_proba = search_lr.best_estimator_.predict_proba(X_train)[:, 1]\n",
        "pr_auc_full = average_precision_score(y_train, y_proba)\n",
        "print(\"PR AUC on full training set:\", pr_auc_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('/Users/victoriayuzova/Data-Science-Projects/Fraud Detection/creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('/Users/victoriayuzova/Data-Science-Projects/Fraud Detection/creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('/Users/victoriayuzova/Data-Science-Projects/Fraud Detection/creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I4o1qe5yWvb"
      },
      "source": [
        "Random Forest with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('ros', RandomOverSampler(random_state=42)),\n",
        "    ('clf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__n_estimators': [100, 300],\n",
        "    'clf__max_depth': [5, 7],\n",
        "    'clf__max_features': ['sqrt'],\n",
        "    'clf__criterion': ['gini']\n",
        "}\n",
        "\n",
        "search_rf = GridSearchCV(\n",
        "    pipe, param_grid, cv=filtered_splits,\n",
        "    scoring='average_precision', n_jobs=-1\n",
        ")\n",
        "search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best PR AUC (mean CV):\", search_rf.best_score_)\n",
        "print(\"Best parameters:\", search_rf.best_params_)\n",
        "\n",
        "y_proba = search_rf.best_estimator_.predict_proba(X_train)[:, 1]\n",
        "pr_auc_full = average_precision_score(y_train, y_proba)\n",
        "print(\"PR AUC on full training set:\", pr_auc_full)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Boosting with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('ros', RandomOverSampler(random_state=42)),\n",
        "    ('clf', GradientBoostingClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__learning_rate': [0.1],                \n",
        "    'clf__n_estimators': [100],                 \n",
        "    'clf__max_depth': [5],                      \n",
        "    \n",
        "}\n",
        "\n",
        "search_gb = GridSearchCV(\n",
        "    pipe, param_grid, cv=filtered_splits,\n",
        "    scoring='average_precision', n_jobs=-1, return_train_score=False\n",
        ")\n",
        "search_gb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best PR AUC (mean CV):\", search_gb.best_score_)\n",
        "print(\"Best parameters:\", search_gb.best_params_)\n",
        "\n",
        "y_proba = search_gb.best_estimator_.predict_proba(X_train)[:, 1]\n",
        "print(\"PR AUC on full training set:\", average_precision_score(y_train, y_proba))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2qpCT-ryWvb"
      },
      "source": [
        "# Compare the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets visualize precision recall curve for all three models\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "# Logistic Regression\n",
        "prec, rec, _ = precision_recall_curve(y_test, search_lr.predict_proba(X_test)[:, 1])\n",
        "ap_lr_test = average_precision_score(y_test, search_lr.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "# Random Forest\n",
        "prec, rec, _ = precision_recall_curve(y_test, search_rf.predict_proba(X_test)[:, 1])\n",
        "ap_rf_test = average_precision_score(y_test, search_rf.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "# Gradient Boosting\n",
        "prec, rec, _ = precision_recall_curve(y_test, search_gb.predict_proba(X_test)[:, 1])\n",
        "ap_gb_test = average_precision_score(y_test, search_gb.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "plt.xlabel(\"Recall - Of all the frauds in reality, how many did you catch?\")\n",
        "plt.ylabel(\"Precision - Of all the users you predicted as frauds, how many actually were frauds?\")\n",
        "plt.legend([\"LR\", \"RF\", \"GB\"])\n",
        "\n",
        "# Print train/test AP\n",
        "print(\"LR AP: Train {:.3f} || Test {:.3f}\".format(search_lr.best_score_, ap_lr_test))\n",
        "print(\"RF AP: Train {:.3f} || Test {:.3f}\".format(search_rf.best_score_, ap_rf_test))\n",
        "print(\"GB AP: Train {:.3f} || Test {:.3f}\".format(search_gb.best_score_, ap_gb_test))\n",
        "plt.hlines(y=0.001727, xmin=0, xmax=1, colors='red', linestyles='dashed', label='Baseline (random)')\n",
        "\n",
        "\n",
        "# looks like the test is pretty different from what we´ve seen in train (not very good?)\n",
        "# so here GB shows to be the best model; we can play with the threasholds and find optimal balance between precision and recall:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here I want to see if I settle for catching 90% of all frauds, how many legitimate users will be wrongly flagged\n",
        "# I think the final decision could depend on company's budget for false positives vs missed frauds\n",
        "\n",
        "# Set your target recall (e.g., catch 90% of all frauds)\n",
        "target_recall = 0.8\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_probs = search_gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Get precision-recall-thresholds\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Find the first index where recall >= target (excluding the last point)\n",
        "valid_idx = np.where(recalls[:-1] >= target_recall)[0]\n",
        "\n",
        "if len(valid_idx) > 0:\n",
        "    idx = valid_idx[-1]  # Take the last one to get highest precision at target recall\n",
        "    threshold = thresholds[idx]\n",
        "    \n",
        "    y_pred = (y_probs >= threshold).astype(int)\n",
        "\n",
        "    cm_gb = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_gb)\n",
        "    disp.plot(cmap='Blues')\n",
        "\n",
        "    print(f\"Threshold for ≥ {target_recall:.2f} recall: {threshold:.3f}\")\n",
        "    print(f\"Recall:    {recalls[idx]:.3f}\")\n",
        "    print(f\"Precision: {precisions[idx]:.3f}\")\n",
        "    \n",
        "    # Business impact analysis\n",
        "    tn, fp, fn, tp = cm_gb.ravel()\n",
        "    total_frauds = tp + fn\n",
        "    total_legitimate = tn + fp\n",
        "    \n",
        "    print(f\"\\nBusiness Impact:\")\n",
        "    print(f\"Total frauds in test set: {total_frauds}\")\n",
        "    print(f\"Frauds caught: {tp} ({tp/total_frauds:.1%})\")\n",
        "    print(f\"Frauds missed: {fn} ({fn/total_frauds:.1%})\")\n",
        "    print(f\"Legitimate users flagged: {fp} ({fp/total_legitimate:.2%} of all legitimate users)\")\n",
        "    print(f\"Legitimate users correctly cleared: {tn}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"No threshold found with recall ≥ {target_recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbggJoZQyWvc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Here I want to see if I settle for getting 60% of predicted fraud right; how many users will be actually hit wrongly and how many fraudsters we willl miss\n",
        "# I think the final desicion could depend on company´s budget for chargebacks  - I would propose missing 10% of fraudulent users (0.885 recall) and being wrong with 50% of predictions; \n",
        "# since these are not that many users in relation to our total population, we are not losing that much\n",
        "# In exchange if we compromised on blocking wrong people (0.1 precision) the win is not that big (?), only 5 additional fraudulent users  or so caught \n",
        "\n",
        "\n",
        "# Set your target precision\n",
        "target_precision = 0.9\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_probs = search_gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Get precision-recall-thresholds\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Find the first index where precision >= target (excluding the last point)\n",
        "valid_idx = np.where(precisions[:-1] >= target_precision)[0]\n",
        "\n",
        "if len(valid_idx) > 0:\n",
        "    idx = valid_idx[0]\n",
        "    threshold = thresholds[idx]\n",
        "    \n",
        "    y_pred = (y_probs >= threshold).astype(int)\n",
        "\n",
        "    cm_gb = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_gb)\n",
        "    disp.plot(cmap='Blues')\n",
        "\n",
        "    print(f\"Threshold for ≥ {target_precision:.2f} precision: {threshold:.3f}\")\n",
        "    print(f\"Precision: {precisions[idx]:.3f}\")\n",
        "    print(f\"Recall:    {recalls[idx]:.3f}\")\n",
        "else:\n",
        "    print(f\"No threshold found with precision ≥ {target_precision}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debug the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first lets check feature importance for all three models\n",
        "\n",
        "lr_imp = np.abs(search_lr.best_estimator_['clf'].coef_).flatten()\n",
        "rf_imp = search_rf.best_estimator_['clf'].feature_importances_\n",
        "gb_imp = search_gb.best_estimator_['clf'].feature_importances_\n",
        "\n",
        "# Helper to get sorted top N indices for a model\n",
        "def top_features(imp, n=15):\n",
        "    idx = np.argsort(imp)[-n:][::-1]\n",
        "    return idx, imp[idx]\n",
        "\n",
        "# Top 15 features for each model (sorted descending)\n",
        "lr_idx, lr_vals = top_features(lr_imp)\n",
        "rf_idx, rf_vals = top_features(rf_imp)\n",
        "gb_idx, gb_vals = top_features(gb_imp)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.barh(range(15), lr_vals[::-1])\n",
        "plt.yticks(range(15), lr_idx[::-1])\n",
        "plt.title(\"Logistic Regression\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.barh(range(15), rf_vals[::-1])\n",
        "plt.yticks(range(15), rf_idx[::-1])\n",
        "plt.title(\"Random Forest\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.barh(range(15), gb_vals[::-1])\n",
        "plt.yticks(range(15), gb_idx[::-1])\n",
        "plt.title(\"Gradient Boosting\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Note from heatmap:\n",
        "## Features with highest positive corr to fraud are 2, 4, 11, 19\n",
        "## Features with highest negative corr to fraud are 3, 9, 10, 12, 14, 17, 18, 20\n",
        "\n",
        "\n",
        "\n",
        "# Notes on feature importance:\n",
        "# Features 13 never appeares as correlated to Fraud in the heatmap but we are seeing it in the feature importance in all three models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict labels (0/1) for each model\n",
        "y_pred_lr = search_lr.predict(X_test)\n",
        "y_pred_rf = search_rf.predict(X_test)\n",
        "y_pred_gb = search_gb.predict(X_test)\n",
        "\n",
        "# Collect row index for comparison\n",
        "row_ids = X_test.index if hasattr(X_test, \"index\") else np.arange(len(X_test))\n",
        "\n",
        "# Identify false positives and false negatives\n",
        "fp_lr = row_ids[(y_pred_lr == 1) & (y_test == 0)]\n",
        "fn_lr = row_ids[(y_pred_lr == 0) & (y_test == 1)]\n",
        "\n",
        "fp_rf = row_ids[(y_pred_rf == 1) & (y_test == 0)]\n",
        "fn_rf = row_ids[(y_pred_rf == 0) & (y_test == 1)]\n",
        "\n",
        "fp_gb = row_ids[(y_pred_gb == 1) & (y_test == 0)]\n",
        "fn_gb = row_ids[(y_pred_gb == 0) & (y_test == 1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overlaps in false positives\n",
        "print(\"FP overlaps:\")\n",
        "print(\"LR & RF:\", len(set(fp_lr).intersection(fp_rf)))\n",
        "print(\"LR & GB:\", len(set(fp_lr).intersection(fp_gb)))\n",
        "print(\"RF & GB:\", len(set(fp_rf).intersection(fp_gb)))\n",
        "\n",
        "# Overlaps in false negatives\n",
        "print(\"\\nFN overlaps:\")\n",
        "print(\"LR & RF:\", len(set(fn_lr).intersection(fn_rf)))\n",
        "print(\"LR & GB:\", len(set(fn_lr).intersection(fn_gb)))\n",
        "print(\"RF & GB:\", len(set(fn_rf).intersection(fn_gb)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets add predictions from all three models to the test set to analize the errors\n",
        "\n",
        "X_test_df = pd.DataFrame(X_test).reset_index(drop=True)\n",
        "\n",
        "y_test_series = pd.Series(y_test, name=\"true_label\").reset_index(drop=True)\n",
        "y_pred_lr_series = pd.Series(y_pred_lr, name=\"pred_lr\")\n",
        "y_pred_rf_series = pd.Series(y_pred_rf, name=\"pred_rf\")\n",
        "y_pred_gb_series = pd.Series(y_pred_gb, name=\"pred_gb\")\n",
        "\n",
        "df_all_preds = pd.concat([\n",
        "    X_test_df,\n",
        "    y_test_series,\n",
        "    y_pred_lr_series,\n",
        "    y_pred_rf_series,\n",
        "    y_pred_gb_series\n",
        "], axis=1)\n",
        "\n",
        "df_all_preds.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Dive GB Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def classify_error(row):\n",
        "    if row['true_label'] == 1 and row['pred_gb'] == 1:\n",
        "        return \"TP\"\n",
        "    elif row['true_label'] == 0 and row['pred_gb'] == 1:\n",
        "        return \"FP\"\n",
        "    elif row['true_label'] == 1 and row['pred_gb'] == 0:\n",
        "        return \"FN\"\n",
        "    else:\n",
        "        return \"TN\"\n",
        "\n",
        "df_all_preds[\"error_type_gb\"] = df_all_preds.apply(classify_error, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features to plot\n",
        "features_to_plot = [13, 3, 11]\n",
        "feature_names = [\"Feature 13\", \"Feature 3\", \"Feature 11\"]\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Loop through features\n",
        "for i, (feature, name) in enumerate(zip(features_to_plot, feature_names)):\n",
        "    sns.boxplot(data=df_all_preds, x=\"error_type_gb\", y=feature, \n",
        "                order=[\"TP\", \"FP\", \"FN\", \"TN\"], ax=axes[i])\n",
        "    axes[i].set_title(f\"{name} Distribution by Error Type\")\n",
        "    axes[i].set_ylabel(f\"{name} Values\")\n",
        "    \n",
        "    # Rotate x-axis labels if needed\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.pairplot(df_all_preds[[3, 13, 11, 'error_type_gb']], hue=\"error_type_gb\", palette=\"Set2\")\n",
        "plt.suptitle(\"Pairplot of Key Features by Error Type\", y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Do false negatives have tight or different values?\n",
        "# Do false positives overlap with true positives?\n",
        "# Is there a range where mistakes consistently happen?\n",
        "\n",
        "\n",
        "df_all_preds[df_all_preds[\"error_type_gb\"]==\"FN\"].describe().transpose().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_all_preds[df_all_preds[\"error_type_gb\"]==\"FP\"].describe().transpose().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Review of overlapping errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataframes for each error type that overlaps between different models\n",
        "\n",
        "df_false_negatives = df_all_preds[(df_all_preds['true_label']==1) & \n",
        "             (df_all_preds['pred_lr']==0) &\n",
        "            (df_all_preds['pred_rf']==0) & \n",
        "            (df_all_preds['pred_gb']==0)]\n",
        "\n",
        "df_false_positives = df_all_preds[(df_all_preds['true_label']==0) & \n",
        "             (df_all_preds['pred_lr']==1) &\n",
        "            (df_all_preds['pred_rf']==1) & \n",
        "            (df_all_preds['pred_gb']==1)]\n",
        "\n",
        "df_true_positives = df_all_preds[(df_all_preds['true_label']==1) & \n",
        "             (df_all_preds['pred_lr']==1) &\n",
        "            (df_all_preds['pred_rf']==1) & \n",
        "            (df_all_preds['pred_gb']==1)]\n",
        "\n",
        "\n",
        "df_true_negatives = df_all_preds[(df_all_preds['true_label']==0) & \n",
        "             (df_all_preds['pred_lr']==0) &\n",
        "            (df_all_preds['pred_rf']==0) & \n",
        "            (df_all_preds['pred_gb']==0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_false_positives.describe().transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Among TP, certain features have huge variability\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "sns.boxplot(data=df_false_positives, ax=axes[0])\n",
        "axes[0].set_title(\"False Positives\")\n",
        "\n",
        "sns.boxplot(data=df_true_positives, ax=axes[1])\n",
        "axes[1].set_title(\"True Positives\")\n",
        "\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trying Ensemble Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "voting_hard = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', search_lr.best_estimator_),\n",
        "        ('rf', search_rf.best_estimator_),\n",
        "        ('gb', search_gb.best_estimator_)\n",
        "    ],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "voting_hard.fit(X_train, y_train)\n",
        "y_pred_hard = voting_hard.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "voting_soft = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', search_lr.best_estimator_),\n",
        "        ('rf', search_rf.best_estimator_),\n",
        "        ('gb', search_gb.best_estimator_)\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[1, 1, 2]  # optional: boost best model\n",
        ")\n",
        "\n",
        "voting_soft.fit(X_train, y_train)\n",
        "y_prob_soft = voting_soft.predict_proba(X_test)[:, 1]\n",
        "y_pred_soft = (y_prob_soft > 0.5).astype(int)  # set your own threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stacked = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', search_lr.best_estimator_),\n",
        "        ('rf', search_rf.best_estimator_),\n",
        "        ('gb', search_gb.best_estimator_)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(),\n",
        "    passthrough=True  # includes original features\n",
        ")\n",
        "\n",
        "stacked.fit(X_train, y_train)\n",
        "y_pred_stacked = stacked.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Logistic Regression\n",
        "prec, rec, _ = precision_recall_curve(y_test, search_lr.predict_proba(X_test)[:, 1])\n",
        "ap_lr_test = average_precision_score(y_test, search_lr.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "# Random Forest\n",
        "prec, rec, _ = precision_recall_curve(y_test, search_rf.predict_proba(X_test)[:, 1])\n",
        "ap_rf_test = average_precision_score(y_test, search_rf.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "# Gradient Boosting\n",
        "prec, rec, _ = precision_recall_curve(y_test, search_gb.predict_proba(X_test)[:, 1])\n",
        "ap_gb_test = average_precision_score(y_test, search_gb.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "# Soft Voting\n",
        "prec, rec, _ = precision_recall_curve(y_test, voting_soft.predict_proba(X_test)[:, 1])\n",
        "ap_soft_test = average_precision_score(y_test, voting_soft.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "# Stacking\n",
        "prec, rec, _ = precision_recall_curve(y_test, stacked.predict_proba(X_test)[:, 1])\n",
        "ap_stack_test = average_precision_score(y_test, stacked.predict_proba(X_test)[:, 1])\n",
        "plt.plot(rec, prec)\n",
        "\n",
        "plt.xlabel(\"Recall - Of all the frauds in reality, how many did you catch?\")\n",
        "plt.ylabel(\"Precision - Of all the users you predicted as frauds, how many actually were frauds?\")\n",
        "plt.legend([\"LR\", \"RF\", \"GB\", \"Soft Voting\", \"Stacking\"])\n",
        "plt.hlines(y=0.001727, xmin=0, xmax=1, colors='red', linestyles='dashed', label='Baseline (random)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"LR AP: Test {:.3f}\".format(ap_lr_test))\n",
        "print(\"RF AP: Test {:.3f}\".format(ap_rf_test))\n",
        "print(\"GB AP: Test {:.3f}\".format(ap_gb_test))\n",
        "print(\"Soft Voting AP: Test {:.3f}\".format(ap_soft_test))\n",
        "print(\"Stacking AP: Test {:.3f}\".format(ap_stack_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 310,
          "sourceId": 23498,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
